{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f8a06c6-7969-4a6f-a2bf-e7ddd32f7a0b",
   "metadata": {},
   "source": [
    "# RL - CIA 1\n",
    "\n",
    "### Run 10 episodes and observe the effect of discounting by changing the values of gamma.\n",
    " \n",
    "For ease of reading and convenience I have added both the gamma values together.\n",
    "\n",
    "**Discounting - reducing the value of future rewards so immediate reward is worth more.**\n",
    "\n",
    "- Discounting with γ = 0.5 keeps the returns below zero for this particular experiment\n",
    "- The returns for γ = 0.5 are not as spread out as in the case of γ = 0.9\n",
    "- γ = 0.9 gives more importance to future rewards than γ = 0.5. Hence when the future exhibits better rewards (more higher values) this is reflected in the return, at the same time when the future exhibits lower rewards (more negative values) the return in significantly smaller.\n",
    "- γ = 0.5 doesn't focus on the future as much as γ = 0.9 so the returns aren't on extremities as seen in γ = 0.9. γ = 0.9 has returns that are more spread out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ec7dc60e-3b7d-4727-a34f-7762128392bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d4cf0c4b-00e9-4938-9b6c-d9da94d19d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given Code\n",
    "\n",
    "ROWS = 3\n",
    "COLS = 4\n",
    "ACTIONS = ['N', 'S', 'E', 'W']\n",
    "REWARDS = { (0, 3): 10,\n",
    " (1, 3): -10 }\n",
    "STEP_REWARD = -1\n",
    "\n",
    "# Terminal states\n",
    "TERMINAL_STATES = [(0, 3), (1, 3)]\n",
    "ACTION_EFFECTS = {\n",
    " 'N': [(-1, 0), (0, -1), (0, 1)],\n",
    " 'S': [(1, 0), (0, -1), (0, 1)],\n",
    " 'E': [(0, 1), (-1, 0), (1, 0)],\n",
    " 'W': [(0, -1), (-1, 0), (1, 0)] \n",
    "}\n",
    "PROBS = [0.8, 0.1, 0.1]\n",
    "\n",
    "def in_bounds(state):\n",
    " \"\"\"Check if the state is inside the grid.\"\"\"\n",
    " r, c = state\n",
    " return 0 <= r < ROWS and 0 <= c < COLS\n",
    "    \n",
    "def move(state, action):\n",
    " \"\"\"Move from current state according to stochastic transitions.\"\"\"\n",
    " effects = ACTION_EFFECTS[action]\n",
    " chosen_effect = random.choices(effects, PROBS)[0]\n",
    " new_r, new_c = state[0] + chosen_effect[0], state[1] + chosen_effect[1]\n",
    " if in_bounds((new_r, new_c)):\n",
    "     return (new_r, new_c)\n",
    " return state # If out of bounds, stay in place\n",
    "    \n",
    "def get_reward(state):\n",
    " \"\"\"Return the reward for a state.\"\"\"\n",
    " return REWARDS.get(state, STEP_REWARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d676ce0c-63d0-46e3-815e-e270aa2315a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function for calculating the discount\n",
    "def discount(rews, gamma):\n",
    "    return np.dot(rews, gamma**np.arange(len(rews))) # multiplies rewards with the gamma value exponents\n",
    "    \n",
    "# Run the experiment\n",
    "def experiment():\n",
    "    # Conducting 10 episodes\n",
    "    for ep in range(10):\n",
    "        state = (0,0) # initial state/starting state is at (0,0)\n",
    "        rews = [] # matrix to store the reward at each time step\n",
    "        states = [state] # Matrix to store the states\n",
    "        \n",
    "        # Do till a terminal state is reached or the max limit 20 is reached\n",
    "        for step in range(20):\n",
    "            action = random.choice(ACTIONS) # Choose a random direction to move (action)\n",
    "            state = (move(state, action)) # Now Based on the transition probabilities choose the state we transiton to from initial state\n",
    "            reward = get_reward(state) # Get reward for the action\n",
    "            rews.append(reward) # Store reward\n",
    "            states.append(state) # Store the state\n",
    "\n",
    "            # Terminate if terminal state is reached\n",
    "            if state in ((0,3), (1,3)):\n",
    "                break\n",
    "    \n",
    "        G_ep_9 = discount(rews, 0.9) # Calculate the Return with gamma value = 0.9\n",
    "        G_ep_5 = discount(rews, 0.5) # Calculate the Return with gamma value = 0.5\n",
    "\n",
    "\n",
    "        # Print the details of the episode - States visited, Rewards earned at each step, Return with gamma 0.9 and 0.5\n",
    "        print(f\"Episode {ep + 1}: \")\n",
    "        print(f\"States visited: {states}\")\n",
    "        print(f\"Rewards: {rews}\")\n",
    "        print(f\"Gamma value 0.9: G_{ep}: {G_ep_9}\")\n",
    "        print(f\"Gamma value 0.5: G_{ep}: {G_ep_5}\")\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e16b2-9860-47a5-a76a-691d3183d272",
   "metadata": {},
   "source": [
    "# Running the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1e77ee29-d7aa-4752-bc16-eb386ed1a12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: \n",
      "States visited: [(0, 0), (1, 0), (1, 1), (1, 0), (0, 0), (0, 0), (0, 1), (0, 1), (0, 1), (0, 1), (0, 0), (0, 1), (1, 1), (0, 1), (0, 0), (0, 0), (0, 0), (1, 0), (0, 0), (0, 1), (0, 0)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Gamma value 0.9: G_0: -8.78423345409431\n",
      "Gamma value 0.5: G_0: -1.9999980926513672\n",
      "\n",
      "Episode 2: \n",
      "States visited: [(0, 0), (0, 0), (0, 0), (1, 0), (1, 0), (2, 0), (1, 0), (0, 0), (1, 0), (1, 1), (0, 1), (0, 2), (0, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10]\n",
      "Gamma value 0.9: G_1: -3.7237880781999997\n",
      "Gamma value 0.5: G_1: -1.994140625\n",
      "\n",
      "Episode 3: \n",
      "States visited: [(0, 0), (0, 0), (0, 0), (1, 0), (1, 0), (2, 0), (1, 0), (0, 0), (1, 0), (1, 0), (1, 1), (0, 1), (0, 1), (1, 1), (1, 2), (0, 2), (0, 2), (0, 2), (0, 1), (1, 1), (1, 0)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Gamma value 0.9: G_2: -8.78423345409431\n",
      "Gamma value 0.5: G_2: -1.9999980926513672\n",
      "\n",
      "Episode 4: \n",
      "States visited: [(0, 0), (0, 0), (0, 1), (0, 2), (1, 2), (2, 2), (2, 1), (2, 1), (1, 1), (1, 2), (2, 2), (2, 2), (2, 2), (1, 2), (1, 1), (0, 1), (0, 1), (1, 1), (1, 0), (2, 0), (2, 0)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Gamma value 0.9: G_3: -8.78423345409431\n",
      "Gamma value 0.5: G_3: -1.9999980926513672\n",
      "\n",
      "Episode 5: \n",
      "States visited: [(0, 0), (0, 1), (1, 1), (2, 1), (2, 2), (2, 3), (1, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -10]\n",
      "Gamma value 0.9: G_4: -10.0\n",
      "Gamma value 0.5: G_4: -2.25\n",
      "\n",
      "Episode 6: \n",
      "States visited: [(0, 0), (0, 0), (0, 1), (0, 2), (0, 3)]\n",
      "Rewards: [-1, -1, -1, 10]\n",
      "Gamma value 0.9: G_5: 4.58\n",
      "Gamma value 0.5: G_5: -0.5\n",
      "\n",
      "Episode 7: \n",
      "States visited: [(0, 0), (1, 0), (1, 1), (1, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 1), (0, 2), (0, 2), (0, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10]\n",
      "Gamma value 0.9: G_6: -4.35140927038\n",
      "Gamma value 0.5: G_6: -1.9970703125\n",
      "\n",
      "Episode 8: \n",
      "States visited: [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (1, 0), (1, 0), (0, 0), (0, 0), (1, 0), (1, 1), (1, 0), (2, 0), (2, 1), (2, 0), (2, 0), (2, 1), (1, 1), (2, 1), (2, 2), (2, 2)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Gamma value 0.9: G_7: -8.78423345409431\n",
      "Gamma value 0.5: G_7: -1.9999980926513672\n",
      "\n",
      "Episode 9: \n",
      "States visited: [(0, 0), (0, 1), (0, 2), (0, 3)]\n",
      "Rewards: [-1, -1, 10]\n",
      "Gamma value 0.9: G_8: 6.200000000000001\n",
      "Gamma value 0.5: G_8: 1.0\n",
      "\n",
      "Episode 10: \n",
      "States visited: [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 1), (1, 1), (1, 0), (2, 0), (1, 0), (1, 1), (2, 1), (2, 2), (2, 1), (1, 1), (2, 1), (2, 0), (2, 1), (1, 1), (2, 1), (2, 1)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Gamma value 0.9: G_9: -8.78423345409431\n",
      "Gamma value 0.5: G_9: -1.9999980926513672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8afbbb-e899-41d5-8c99-be4a41fd7c8f",
   "metadata": {},
   "source": [
    "## I wanted to see more values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "373c3ae7-5b09-4386-ada9-9892d762af9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: \n",
      "States visited: [(0, 0), (1, 0), (1, 1), (1, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (2, 0), (1, 0), (1, 1), (0, 1), (0, 1), (1, 1), (1, 2), (0, 2), (0, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10]\n",
      "Gamma value 0.9: G_0: -6.293959622296319\n",
      "Gamma value 0.5: G_0: -1.99981689453125\n",
      "\n",
      "Episode 2: \n",
      "States visited: [(0, 0), (1, 0), (1, 0), (1, 0), (0, 0), (0, 1), (0, 2), (0, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, 10]\n",
      "Gamma value 0.9: G_1: 0.6288200000000006\n",
      "Gamma value 0.5: G_1: -1.8125\n",
      "\n",
      "Episode 3: \n",
      "States visited: [(0, 0), (1, 0), (0, 0), (0, 0), (0, 0), (1, 0), (2, 0), (2, 1), (2, 2), (2, 2), (1, 2), (1, 1), (1, 2), (1, 1), (0, 1), (0, 0), (0, 0), (1, 0), (2, 0), (2, 0), (1, 0)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Gamma value 0.9: G_2: -8.78423345409431\n",
      "Gamma value 0.5: G_2: -1.9999980926513672\n",
      "\n",
      "Episode 4: \n",
      "States visited: [(0, 0), (1, 0), (1, 1), (1, 2), (1, 3)]\n",
      "Rewards: [-1, -1, -1, -10]\n",
      "Gamma value 0.9: G_3: -10.000000000000002\n",
      "Gamma value 0.5: G_3: -3.0\n",
      "\n",
      "Episode 5: \n",
      "States visited: [(0, 0), (0, 1), (0, 0), (0, 0), (1, 0), (2, 0), (2, 0), (2, 0), (2, 1), (2, 1), (2, 2), (2, 3), (2, 2), (1, 2), (0, 2), (0, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10]\n",
      "Gamma value 0.9: G_4: -5.424641509007801\n",
      "Gamma value 0.5: G_4: -1.999267578125\n",
      "\n",
      "Episode 6: \n",
      "States visited: [(0, 0), (0, 0), (1, 0), (0, 0), (0, 0), (1, 0), (1, 1), (2, 1), (2, 2), (2, 3), (2, 3), (2, 3), (2, 3), (2, 2), (2, 3), (2, 3), (2, 3), (2, 2), (1, 2), (2, 2), (1, 2)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Gamma value 0.9: G_5: -8.78423345409431\n",
      "Gamma value 0.5: G_5: -1.9999980926513672\n",
      "\n",
      "Episode 7: \n",
      "States visited: [(0, 0), (0, 0), (1, 0), (0, 0), (1, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (1, 0), (1, 1), (1, 0), (2, 0), (2, 1), (2, 2), (2, 3), (2, 3), (1, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10]\n",
      "Gamma value 0.9: G_6: -10.000000000000002\n",
      "Gamma value 0.5: G_6: -2.000030517578125\n",
      "\n",
      "Episode 8: \n",
      "States visited: [(0, 0), (0, 0), (0, 0), (0, 1), (0, 2), (0, 1), (0, 0), (0, 0), (0, 0), (0, 1), (1, 1), (0, 1), (1, 1), (1, 0), (1, 0), (0, 0), (0, 1), (0, 1), (1, 1), (1, 0), (1, 0)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Gamma value 0.9: G_7: -8.78423345409431\n",
      "Gamma value 0.5: G_7: -1.9999980926513672\n",
      "\n",
      "Episode 9: \n",
      "States visited: [(0, 0), (0, 0), (0, 0), (1, 0), (1, 0), (1, 0), (0, 0), (1, 0), (1, 1), (0, 1), (0, 2), (1, 2), (1, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10]\n",
      "Gamma value 0.9: G_8: -10.000000000000002\n",
      "Gamma value 0.5: G_8: -2.00390625\n",
      "\n",
      "Episode 10: \n",
      "States visited: [(0, 0), (0, 0), (0, 1), (0, 2), (1, 2), (2, 2), (2, 3), (1, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -10]\n",
      "Gamma value 0.9: G_9: -10.0\n",
      "Gamma value 0.5: G_9: -2.125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d87270cd-1435-42f7-8db3-4a2b047805fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: \n",
      "States visited: [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (1, 0), (1, 1), (1, 2), (1, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -10]\n",
      "Gamma value 0.9: G_0: -10.0\n",
      "Gamma value 0.5: G_0: -2.0625\n",
      "\n",
      "Episode 2: \n",
      "States visited: [(0, 0), (1, 0), (1, 1), (2, 1), (2, 2), (2, 3), (2, 3), (2, 3), (1, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -10]\n",
      "Gamma value 0.9: G_1: -10.0\n",
      "Gamma value 0.5: G_1: -2.0625\n",
      "\n",
      "Episode 3: \n",
      "States visited: [(0, 0), (0, 0), (0, 1), (0, 2), (0, 1), (0, 1), (0, 1), (0, 1), (1, 1), (2, 1), (2, 1), (1, 1), (2, 1), (2, 2), (2, 1), (2, 2), (1, 2), (0, 2), (0, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10]\n",
      "Gamma value 0.9: G_2: -6.664563660066687\n",
      "Gamma value 0.5: G_2: -1.999908447265625\n",
      "\n",
      "Episode 4: \n",
      "States visited: [(0, 0), (1, 0), (2, 0), (1, 0), (2, 0), (2, 1), (1, 1), (2, 1), (1, 1), (0, 1), (0, 0), (0, 0), (1, 0), (0, 0), (0, 0), (0, 1), (0, 2), (1, 2), (2, 2), (1, 2), (0, 2)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Gamma value 0.9: G_3: -8.78423345409431\n",
      "Gamma value 0.5: G_3: -1.9999980926513672\n",
      "\n",
      "Episode 5: \n",
      "States visited: [(0, 0), (0, 1), (0, 0), (1, 0), (2, 0), (2, 0), (2, 1), (1, 1), (0, 1), (0, 0), (0, 0), (0, 0), (0, 0), (1, 0), (1, 0), (1, 1), (0, 1), (0, 2), (0, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10]\n",
      "Gamma value 0.9: G_4: -6.664563660066687\n",
      "Gamma value 0.5: G_4: -1.999908447265625\n",
      "\n",
      "Episode 6: \n",
      "States visited: [(0, 0), (1, 0), (1, 0), (2, 0), (1, 0), (2, 0), (2, 1), (1, 1), (0, 1), (0, 1), (0, 1), (0, 2), (0, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10]\n",
      "Gamma value 0.9: G_5: -3.7237880781999997\n",
      "Gamma value 0.5: G_5: -1.994140625\n",
      "\n",
      "Episode 7: \n",
      "States visited: [(0, 0), (1, 0), (2, 0), (2, 0), (1, 0), (2, 0), (2, 0), (1, 0), (1, 0), (1, 0), (2, 0), (2, 0), (2, 1), (2, 1), (2, 1), (2, 1), (2, 0), (2, 0), (2, 1), (2, 0), (2, 1)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Gamma value 0.9: G_6: -8.78423345409431\n",
      "Gamma value 0.5: G_6: -1.9999980926513672\n",
      "\n",
      "Episode 8: \n",
      "States visited: [(0, 0), (0, 0), (1, 0), (0, 0), (0, 0), (0, 1), (0, 2), (1, 2), (1, 1), (1, 0), (1, 1), (1, 2), (1, 3)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -10]\n",
      "Gamma value 0.9: G_7: -10.000000000000002\n",
      "Gamma value 0.5: G_7: -2.00390625\n",
      "\n",
      "Episode 9: \n",
      "States visited: [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 1), (0, 2), (0, 1), (1, 1), (1, 2), (2, 2), (1, 2), (0, 2), (0, 1), (1, 1), (1, 0), (0, 0), (0, 0), (0, 1), (1, 1), (1, 0)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Gamma value 0.9: G_8: -8.78423345409431\n",
      "Gamma value 0.5: G_8: -1.9999980926513672\n",
      "\n",
      "Episode 10: \n",
      "States visited: [(0, 0), (0, 0), (0, 1), (0, 1), (1, 1), (0, 1), (1, 1), (0, 1), (0, 1), (0, 0), (1, 0), (2, 0), (2, 1), (2, 1), (2, 2), (2, 2), (2, 1), (2, 1), (1, 1), (0, 1), (0, 2)]\n",
      "Rewards: [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Gamma value 0.9: G_9: -8.78423345409431\n",
      "Gamma value 0.5: G_9: -1.9999980926513672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
